---
title: Smart Assistant Interaction through Visual Gesture Recognition using a Kinect Sensor
gitURL: https://github.com/enricobu96/vcs-project/releases
documentURL: https://github.com/enricobu96/vcs-project/releases/download/v1.0.0/VCS-project_Buratto-Sciacco.pdf
coverURL: 
websiteURL:
coauthors: [Enrico Buratto]
technologies: [Python]
tags: [unipd,msc]
date: 2021-07-01
draft: false
---

> Progetto di Vision and Cognitive Services, in collaborazione con Enrico Buratto.

__Abstract:__ In the last decade, the raise of smart assistant devices played a key role for the everyday life as they help to retrieve information from the web, manage other home devices and execute routines independently. Even though the vocal interaction is getting more and more efficient, not always the commands are interpreted correctly. Moreover, looking at deaf people and/or people with speech disabilities, the effective interaction is drastically reduced, thus limiting the use of this kind of devices. The main goal of this project is to use the visible body parts of a person to interact with smart devices like Google Home through human pose estimation and gesture recognition. During our work we compared two different solutions – Google MediaPipe and NiTE2 middleware for Kinect –, and for both of these use-cases we tried different classification algorithms: multinomial logistic regression, ridge regression, random forest, support vector machines and multilayer perceptron. Through intensive trial and error experiments we were able to get F1-scores in a range between 0.71 and 0.97 for the different use-cases and classification algorithms, with an average accuracy never below 0.70.